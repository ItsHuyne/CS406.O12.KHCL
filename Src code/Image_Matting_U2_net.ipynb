{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5827204,"sourceType":"datasetVersion","datasetId":2854714},{"sourceId":7297315,"sourceType":"datasetVersion","datasetId":4232865},{"sourceId":7400987,"sourceType":"datasetVersion","datasetId":4210728}],"dockerImageVersionId":30476,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Add\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom glob import glob\n\ndef conv_block(inputs, out_ch, rate=1):\n    x = Conv2D(out_ch, 3, padding=\"same\", dilation_rate=1)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef RSU_L(inputs, out_ch, int_ch, num_layers, rate=2):\n    \"\"\" Initial Conv \"\"\"\n    x = conv_block(inputs, out_ch)\n    init_feats = x\n\n    \"\"\" Encoder \"\"\"\n    skip = []\n    x = conv_block(x, int_ch)\n    skip.append(x)\n\n    for i in range(num_layers-2):\n        x = MaxPool2D((2, 2))(x)\n        x = conv_block(x, int_ch)\n        skip.append(x)\n\n    \"\"\" Bridge \"\"\"\n    x = conv_block(x, int_ch, rate=rate)\n\n    \"\"\" Decoder \"\"\"\n    skip.reverse()\n\n    x = Concatenate()([x, skip[0]])\n    x = conv_block(x, int_ch)\n\n    for i in range(num_layers-3):\n        x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n        x = Concatenate()([x, skip[i+1]])\n        x = conv_block(x, int_ch)\n\n    x = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(x)\n    x = Concatenate()([x, skip[-1]])\n    x = conv_block(x, out_ch)\n\n    \"\"\" Add \"\"\"\n    x = Add()([x, init_feats])\n    return x\n\ndef RSU_4F(inputs, out_ch, int_ch):\n    \"\"\" Initial Conv \"\"\"\n    x0 = conv_block(inputs, out_ch, rate=1)\n\n    \"\"\" Encoder \"\"\"\n    x1 = conv_block(x0, int_ch, rate=1)\n    x2 = conv_block(x1, int_ch, rate=2)\n    x3 = conv_block(x2, int_ch, rate=4)\n\n    \"\"\" Bridge \"\"\"\n    x4 = conv_block(x3, int_ch, rate=8)\n\n    \"\"\" Decoder \"\"\"\n    x = Concatenate()([x4, x3])\n    x = conv_block(x, int_ch, rate=4)\n\n    x = Concatenate()([x, x2])\n    x = conv_block(x, int_ch, rate=2)\n\n    x = Concatenate()([x, x1])\n    x = conv_block(x, out_ch, rate=1)\n\n    \"\"\" Addition \"\"\"\n    x = Add()([x, x0])\n    return x\n\ndef u2net(input_shape, out_ch, int_ch, num_classes=1):\n    \"\"\" Input Layer \"\"\"\n    inputs = Input(input_shape)\n    s0 = inputs\n\n    \"\"\" Encoder \"\"\"\n    s1 = RSU_L(s0, out_ch[0], int_ch[0], 7)\n    p1 = MaxPool2D((2, 2))(s1)\n\n    s2 = RSU_L(p1, out_ch[1], int_ch[1], 6)\n    p2 = MaxPool2D((2, 2))(s2)\n\n    s3 = RSU_L(p2, out_ch[2], int_ch[2], 5)\n    p3 = MaxPool2D((2, 2))(s3)\n\n    s4 = RSU_L(p3, out_ch[3], int_ch[3], 4)\n    p4 = MaxPool2D((2, 2))(s4)\n\n    s5 = RSU_4F(p4, out_ch[4], int_ch[4])\n    p5 = MaxPool2D((2, 2))(s5)\n\n    \"\"\" Bridge \"\"\"\n    b1 = RSU_4F(p5, out_ch[5], int_ch[5])\n    b2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(b1)\n\n    \"\"\" Decoder \"\"\"\n    d1 = Concatenate()([b2, s5])\n    d1 = RSU_4F(d1, out_ch[6], int_ch[6])\n    u1 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d1)\n\n    d2 = Concatenate()([u1, s4])\n    d2 = RSU_L(d2, out_ch[7], int_ch[7], 4)\n    u2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d2)\n\n    d3 = Concatenate()([u2, s3])\n    d3 = RSU_L(d3, out_ch[8], int_ch[8], 5)\n    u3 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d3)\n\n    d4 = Concatenate()([u3, s2])\n    d4 = RSU_L(d4, out_ch[9], int_ch[9], 6)\n    u4 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(d4)\n\n    d5 = Concatenate()([u4, s1])\n    d5 = RSU_L(d5, out_ch[10], int_ch[10], 7)\n\n    \"\"\" Side Outputs \"\"\"\n    y1 = Conv2D(num_classes, 3, padding=\"same\")(d5)\n\n    y2 = Conv2D(num_classes, 3, padding=\"same\")(d4)\n    y2 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\")(y2)\n\n    y3 = Conv2D(num_classes, 3, padding=\"same\")(d3)\n    y3 = UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(y3)\n\n    y4 = Conv2D(num_classes, 3, padding=\"same\")(d2)\n    y4 = UpSampling2D(size=(8, 8), interpolation=\"bilinear\")(y4)\n\n    y5 = Conv2D(num_classes, 3, padding=\"same\")(d1)\n    y5 = UpSampling2D(size=(16, 16), interpolation=\"bilinear\")(y5)\n\n    y6 = Conv2D(num_classes, 3, padding=\"same\")(b1)\n    y6 = UpSampling2D(size=(32, 32), interpolation=\"bilinear\")(y6)\n\n    y0 = Concatenate()([y1, y2, y3, y4, y5, y6])\n    y0 = Conv2D(num_classes, 3, padding=\"same\")(y0)\n\n    y0 = Activation(\"sigmoid\", name=\"y0\")(y0)\n    y1 = Activation(\"sigmoid\", name=\"y1\")(y1)\n    y2 = Activation(\"sigmoid\", name=\"y2\")(y2)\n    y3 = Activation(\"sigmoid\", name=\"y3\")(y3)\n    y4 = Activation(\"sigmoid\", name=\"y4\")(y4)\n    y5 = Activation(\"sigmoid\", name=\"y5\")(y5)\n    y6 = Activation(\"sigmoid\", name=\"y6\")(y6)\n\n    model = tf.keras.models.Model(inputs, outputs=[y0, y1, y2, y3, y4, y5, y6])\n    return model\n\ndef build_u2net(input_shape, num_classes=1):\n    out_ch = [64, 128, 256, 512, 512, 512, 512, 256, 128, 64, 64]\n    int_ch = [32, 32, 64, 128, 256, 256, 256, 128, 64, 32, 16]\n    model = u2net(input_shape, out_ch, int_ch, num_classes=num_classes)\n    return model\n\ndef build_u2net_lite(input_shape, num_classes=1):\n    out_ch = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n    int_ch = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n    model = u2net(input_shape, out_ch, int_ch, num_classes=num_classes)\n    return model\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T08:54:19.676943Z","iopub.execute_input":"2024-01-26T08:54:19.677324Z","iopub.status.idle":"2024-01-26T08:54:28.898109Z","shell.execute_reply.started":"2024-01-26T08:54:19.677292Z","shell.execute_reply":"2024-01-26T08:54:28.897250Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#pip install \"numpy>=1.16.5,<1.23.0\"","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.900074Z","iopub.execute_input":"2024-01-26T08:54:28.900730Z","iopub.status.idle":"2024-01-26T08:54:28.907134Z","shell.execute_reply.started":"2024-01-26T08:54:28.900702Z","shell.execute_reply":"2024-01-26T08:54:28.906264Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data loader","metadata":{}},{"cell_type":"code","source":"\"\"\" Global parameters \"\"\"\nH = 512\nW = 512\n\ndef load_dataset(path, split=0.1):\n    train_x_path = os.path.join(path, \"train\", \"original\")\n    train_y_path = os.path.join(path, \"train\", \"mask\")\n    train_x = sorted([os.path.join(train_x_path, x) for x in os.listdir(train_x_path)])\n    train_y = sorted([os.path.join(train_y_path, x) for x in os.listdir(train_y_path)])\n\n    valid_x_path = os.path.join(path, \"validation\", \"image\")\n    valid_y_path = os.path.join(path, \"validation\", \"mask\")\n    valid_x = sorted([os.path.join(valid_x_path, x) for x in os.listdir(valid_x_path)])\n    valid_y = sorted([os.path.join(valid_y_path, x) for x in os.listdir(valid_y_path)])\n\n    return (train_x, train_y), (valid_x, valid_y)\n\n\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n#     x = x / 255.0\n#     x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n#     x = x.astype(np.float32)\n#     x = np.expand_dims(x, axis=-1)\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.908127Z","iopub.execute_input":"2024-01-26T08:54:28.908434Z","iopub.status.idle":"2024-01-26T08:54:28.937764Z","shell.execute_reply.started":"2024-01-26T08:54:28.908410Z","shell.execute_reply":"2024-01-26T08:54:28.936947Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.ElasticTransform(always_apply=False, p=0.2, alpha=1.0, sigma=19.74, alpha_affine=20.39, interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, approximate=False, same_dxdy=False),\n    A.CLAHE(always_apply=False, p=0.4, clip_limit=(1, 4), tile_grid_size=(8, 8))\n])\n\n\ndef tf_parse(x, y):\n    def _parse(x, y, isTrain= False):\n        x = read_image(x)\n        y = read_mask(y)\n        if isTrain:\n            transformed = transform(image=x, mask=y)\n            x = transformed['image']\n            y = transformed['mask']\n            \n        x = x / 255.0\n        x = x.astype(np.float32)\n        \n        y = y / 255.0\n        y = y.astype(np.float32)\n        y = np.expand_dims(y, axis=-1)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n    x.set_shape([H, W, 3])\n    y.set_shape([H, W, 1])\n    return x, y\n\n\ndef tf_dataset(X, Y, batch=2):\n    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n    ds = ds.map(tf_parse).batch(batch).prefetch(10)\n    return ds\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.938929Z","iopub.execute_input":"2024-01-26T08:54:28.939554Z","iopub.status.idle":"2024-01-26T08:54:28.952555Z","shell.execute_reply.started":"2024-01-26T08:54:28.939521Z","shell.execute_reply":"2024-01-26T08:54:28.951643Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\nfrom keras import layers\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\n\ndef alpha_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))\n\n\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\ndef gradient_loss(y_true, y_pred):\n    grad_true_x = tf.abs(y_true[:, :-1, :] - y_true[:, 1:, :])\n    grad_true_y = tf.abs(y_true[:, :, :-1] - y_true[:, :, 1:])\n    grad_pred_x = tf.abs(y_pred[:, :-1, :] - y_pred[:, 1:, :])\n    grad_pred_y = tf.abs(y_pred[:, :, :-1] - y_pred[:, :, 1:])\n    return tf.reduce_mean(tf.square(grad_true_x - grad_pred_x)) + tf.reduce_mean(tf.square(grad_true_y - grad_pred_y))\n\n\ndef custom_loss(y_true, y_pred):\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n    return alpha_loss(y_true, y_pred) + ssim_loss(y_true, y_pred) + binary_crossentropy(y_true, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.955539Z","iopub.execute_input":"2024-01-26T08:54:28.956261Z","iopub.status.idle":"2024-01-26T08:54:28.967175Z","shell.execute_reply.started":"2024-01-26T08:54:28.956229Z","shell.execute_reply":"2024-01-26T08:54:28.966247Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def SAD(y_true, y_pred):\n\n    # Convert images to float32\n    img1 = tf.image.convert_image_dtype(y_true, tf.float32)\n    img2 = tf.image.convert_image_dtype(y_pred, tf.float32)\n\n    # Calculate absolute differences\n    abs_diff = tf.abs(tf.subtract(img1, img2))\n\n    # Sum of absolute differences\n    sad = tf.reduce_sum(abs_diff)\n\n    return sad\n\n\ndef MSE(y_true, y_pred):\n\n    # Convert images to float32\n    img1 = tf.image.convert_image_dtype(y_true, tf.float32)\n    img2 = tf.image.convert_image_dtype(y_pred, tf.float32)\n\n    # Calculate squared differences\n    squared_diff = tf.square(tf.subtract(img1, img2))\n\n    # Mean Squared Error\n    mse = tf.reduce_mean(squared_diff)\n\n    return mse\n\n\ndef MAD(y_true, y_pred):\n    # Convert images to float32\n    img1 = tf.image.convert_image_dtype(y_true, tf.float32)\n    img2 = tf.image.convert_image_dtype(y_pred, tf.float32)\n\n    # Calculate absolute differences\n    abs_diff = tf.abs(tf.subtract(img1, img2))\n\n    # Mean of Absolute Differences\n    mad = tf.reduce_mean(abs_diff)\n\n    return mad","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.968408Z","iopub.execute_input":"2024-01-26T08:54:28.968693Z","iopub.status.idle":"2024-01-26T08:54:28.981977Z","shell.execute_reply.started":"2024-01-26T08:54:28.968669Z","shell.execute_reply":"2024-01-26T08:54:28.981143Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\n\n\"\"\" Hyperparameters \"\"\"\nbatch_size = 4\nlr = 1e-7\nnum_epochs = 1\nmodel_path = \"U2Net_custom_loss.h5\"\ncsv_path = \"log.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.983093Z","iopub.execute_input":"2024-01-26T08:54:28.984013Z","iopub.status.idle":"2024-01-26T08:54:28.991819Z","shell.execute_reply.started":"2024-01-26T08:54:28.983988Z","shell.execute_reply":"2024-01-26T08:54:28.990994Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/p3m10k-portraitsegmentation\"\n(train_x, train_y), (valid_x, valid_y) = load_dataset(dataset_path)\n\nprint(f\"Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n\ntrain_dataset = tf_dataset(train_x, train_y, batch=batch_size)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:28.992945Z","iopub.execute_input":"2024-01-26T08:54:28.993257Z","iopub.status.idle":"2024-01-26T08:54:32.475490Z","shell.execute_reply.started":"2024-01-26T08:54:28.993225Z","shell.execute_reply":"2024-01-26T08:54:32.474529Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Train: 11321 - 11321\nValid: 1200 - 1200\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.metrics import MeanSquaredError\nmodel = build_u2net((H, W, 3))\nmse_metric = MeanSquaredError(name='mse')\nmodel.load_weights('/kaggle/input/weight/U2Net_AutoMattingData-0.6424-weights-09.h5')\nmodel.compile(loss=custom_loss, optimizer=Adam(lr), metrics=[MAD, SAD, MSE])\n\n# callbacks = [\n#     ModelCheckpoint(filepath='U2Net_AutoMattingData-{val_loss:.4f}-weights-{epoch:02d}.h5', verbose=1, save_best_only=True),\n#     ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-12, verbose=1),\n#     CSVLogger(csv_path),\n#     EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n# ]\n\n# history = model.fit(\n#     train_dataset,\n#     epochs=num_epochs,\n#     validation_data=valid_dataset,\n#     callbacks=callbacks \n# )\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:32.476834Z","iopub.execute_input":"2024-01-26T08:54:32.477237Z","iopub.status.idle":"2024-01-26T08:54:37.898826Z","shell.execute_reply.started":"2024-01-26T08:54:32.477183Z","shell.execute_reply":"2024-01-26T08:54:37.897856Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model.evaluate(valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:54:37.901055Z","iopub.execute_input":"2024-01-26T08:54:37.901483Z","iopub.status.idle":"2024-01-26T08:56:09.194631Z","shell.execute_reply.started":"2024-01-26T08:54:37.901445Z","shell.execute_reply":"2024-01-26T08:56:09.193576Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"300/300 [==============================] - 91s 252ms/step - loss: 0.6424 - y0_loss: 0.0701 - y1_loss: 0.0705 - y2_loss: 0.0740 - y3_loss: 0.0816 - y4_loss: 0.0922 - y5_loss: 0.1088 - y6_loss: 0.1452 - y0_MAD: 0.0097 - y0_SAD: 10217.5605 - y0_MSE: 0.0062 - y1_MAD: 0.0097 - y1_SAD: 10163.7314 - y1_MSE: 0.0062 - y2_MAD: 0.0101 - y2_SAD: 10584.9727 - y2_MSE: 0.0064 - y3_MAD: 0.0109 - y3_SAD: 11478.6162 - y3_MSE: 0.0069 - y4_MAD: 0.0125 - y4_SAD: 13104.3574 - y4_MSE: 0.0079 - y5_MAD: 0.0149 - y5_SAD: 15660.8652 - y5_MSE: 0.0094 - y6_MAD: 0.0211 - y6_SAD: 22172.4766 - y6_MSE: 0.0132\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[0.6424411535263062,\n 0.07014510780572891,\n 0.07048436999320984,\n 0.07400575280189514,\n 0.08164706081151962,\n 0.0921558290719986,\n 0.10877583920955658,\n 0.1452268660068512,\n 0.009744225069880486,\n 10217.560546875,\n 0.006238456349819899,\n 0.009692889638245106,\n 10163.7314453125,\n 0.006247739773243666,\n 0.010094616562128067,\n 10584.97265625,\n 0.006421877536922693,\n 0.01094686146825552,\n 11478.6162109375,\n 0.0069298529997467995,\n 0.01249728910624981,\n 13104.357421875,\n 0.007880646735429764,\n 0.014935364946722984,\n 15660.865234375,\n 0.009436163119971752,\n 0.021145321428775787,\n 22172.4765625,\n 0.0132065424695611]"},"metadata":{}}]},{"cell_type":"code","source":"print(history.history.keys())","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:56:09.195964Z","iopub.execute_input":"2024-01-26T08:56:09.196347Z","iopub.status.idle":"2024-01-26T08:56:09.439791Z","shell.execute_reply.started":"2024-01-26T08:56:09.196313Z","shell.execute_reply":"2024-01-26T08:56:09.438541Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"],"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['mean_squared_error'])\nplt.plot(history.history['val_mean_squared_error'])\nplt.title('Model MSE')\nplt.ylabel('MSE')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T08:56:09.440628Z","iopub.status.idle":"2024-01-26T08:56:09.440948Z","shell.execute_reply.started":"2024-01-26T08:56:09.440790Z","shell.execute_reply":"2024-01-26T08:56:09.440804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}